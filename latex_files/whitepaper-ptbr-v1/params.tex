\begin{table}[H]
\caption{Methods settings}
\centering
\begin{tabular}{lll}

Methods                                                             & \begin{tabular}[c]{@{}l@{}}DQN with experience replay\\and target network\end{tabular}                                                                                                                                                                               & \begin{tabular}[c]{@{}l@{}}DDPG: Actor-critic, experience replay and\\ target network\end{tabular}                                                                                                                                                                                                                                                                                                    \\\hline
\begin{tabular}[c]{@{}l@{}}ANN's\\ architecture\end{tabular}      & \begin{tabular}[c]{@{}l@{}}Feedforward, Input: states,\\ 4 layers: {[}256,128,64,33{]}\\ neurons, ReLU activation on\\hidden layers, linear on\\output, Output: action values,\\ Optimization: Adam, Learning\\ rate $1\mathrm{e}{-3}$\end{tabular}         & \begin{tabular}[c]{@{}l@{}}Actor: feedforward, Input: states, 3 layers:\\ {[}400,300,2{]} neurons, ReLU activation on \\hidden layers, softsign on output, Output:\\action, Optimization: Adam, Learning\\ rate $1\mathrm{e}{-4}$\\ \\ Critic: feedforward, Input: actions, states,\\ 3 layers: {[}400,300,1{]} neurons, ReLU activation\\ on hidden layers, linear on output, Output:\\ boolean (dimension = actions), Optimization:\\ Adam, Learning rate $1\mathrm{e}{-3}$\end{tabular} \\ %\hline
Training                                                             & \begin{tabular}[c]{@{}l@{}}Steps: 400000, $\gamma=0.99$\\ Policy: $\varepsilon$-greedy, $\varepsilon$ linear\\ annealed from 1 to 0.1,\\ Experience replay memory:\\20000 transitions, Hard Target\\ Model Update: $C=1000$\end{tabular} & \begin{tabular}[c]{@{}l@{}}Steps: 600000, $\gamma=0.99$, Stochastic process:\\Ornstein-Uhlenbeck ($\theta=0.3$, $\mu=0$, $\sigma=0.3$),\\ Experience replay memory: 20000 transitions\\ Soft Target Model Update: $C=1\mathrm{e}{-2}$\end{tabular}                                                                                                                     \\ %\hline
\begin{tabular}[c]{@{}l@{}}Transfer\\ Learning\end{tabular} & \begin{tabular}[c]{@{}l@{}}SPM with 50000 steps, $\varepsilon$ linear\\annealed from 0.1 to 0.01\end{tabular}                                                                                                                                                      & \begin{tabular}[c]{@{}l@{}}SPM with 100000 steps, parameters equal \\to training\end{tabular}                                                                                                                                                                                                                                                                         
\end{tabular}
\end{table}